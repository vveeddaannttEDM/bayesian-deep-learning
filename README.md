
# Bayesian Computation in Deep Learning

This repository contains the paper **"Bayesian Computation in Deep Learning"** by Wenlong Chen, Bolian Li, Ruqi Zhang, and Yingzhen Li.

## Overview
Bayesian methods have gained significant traction in deep learning, particularly for:
- Improving predictive accuracy via **Bayesian model averaging**
- **Uncertainty quantification** in safety-critical applications
- Training **Bayesian Neural Networks (BNNs)**
- Enhancing **deep generative models** (e.g., VAEs, EBMs, and diffusion models)

This repository includes:
- The original paper `2502.18300v1.pdf`
- A brief summary of key methods explored in the paper
- Implementation guides and references for Bayesian inference techniques in deep learning

## Contents
- `2502.18300v1.pdf` - The research paper
- `README.md` - This file

## Topics Covered
1. **Bayesian Neural Networks (BNNs)**
   - Markov Chain Monte Carlo (MCMC)
   - Stochastic Gradient MCMC
   - Variational Inference (VI)
2. **Deep Generative Models**
   - Energy-Based Models (EBMs)
   - Score-Based Models (SBMs)
   - Variational Autoencoders (VAEs)
   - Diffusion Models
3. **Hybrid Approaches**
   - Combining VI with MCMC for posterior inference
   - Amortized inference for scalable Bayesian deep learning

## How to Use
1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/bayesian-deep-learning.git
   ```
2. Open and read the paper in `2502.18300v1.pdf`.
3. Explore Bayesian deep learning methods through the references and provided materials.

## References
- Original paper: [arXiv:2502.18300v1](https://arxiv.org/abs/2502.18300)
- Related works on Bayesian deep learning:
  - Blundell et al. (2015) "Weight Uncertainty in Neural Networks"
  - Kingma & Welling (2014) "Auto-Encoding Variational Bayes"
  - Welling & Teh (2011) "Bayesian Learning via Stochastic Gradient Langevin Dynamics"

## License
This repository is for academic and research purposes. Please cite the original authors if you use this work in your research.

## Contact
For questions or discussions, feel free to open an issue or reach out to the original authors.
